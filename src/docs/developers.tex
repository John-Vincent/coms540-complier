\documentclass[11pt]{article}
\usepackage{mathtools}
\usepackage{mdframed}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{fancyhdr}
\usepackage{lastpage}


%edit this for each class
\newcommand\name{John Collin Vincent}
\newcommand\classname{COM S 540}
\newcommand\assignment{Part 1}


\newcounter{excounter}
\setcounter{excounter}{1}
\newcommand\ques[2]{\vskip 1em  \noindent\textbf{\arabic{excounter}\addtocounter{excounter}{1}.} \emph{#1} \noindent#2}
\newenvironment{question}{\ques{}\begin{quote}}{\end{quote}}
\newenvironment{subquestion}[1]{#1) \begin{quote}}{\end{quote}}

\pagestyle{fancy}
\rfoot{\name, page \thepage/\pageref{LastPage}}
\cfoot{}
\rhead{}
\lhead{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}


\begin{document}


  {\bf \classname \hspace{1cm} \assignment\hfill \name}
  \vskip 2em
    
    \tableofcontents

    \clearpage

    \section{Introduction}
        I am submitting this assignment 1 day late because I was told that there would be a 5 to 10 point penalty for doing so.
        taking the extra day has allowed me to implement all of the features for this assignment include all of the extra credit
        features. It also gave me time to track down all memory leaks and fix my memory management. I tested this code with its own 
        source code, and since the source code had to correctly compile before I could test there was no testing on invalid C sources.
        Even with the extra day I don't have time to do complete testing so I'm curious to see how well my program works with
        your test cases.
    
    \section{Main}
        The main file in this program interprets the command line arguments, sets the runtime options in a global variable called 
        program\_optons, and copies all input files into a string array. Based on the options it then makes calls to the different 
        stages of the compiler(lexer, parser, type analizer, intermediate code generate, and target language generator).
        As of right now only the lexer works and the rest print errors.

    \section{Lex File}
        The lex file makes tokens for each of the tokens specified in the assignment document.
        It ignores c++ style comments by using a comment start state that has a rule for '.' that does nothing.
        The comment start state has a rule that matches "*/" which then sets the state back to the initial to resume
        normal tokenization. This file also defines a new global variable to go along with yytext called yyline.
        This counter is incremented for each newline that is read. It must be reset manually when the file is changed
        as the lex rules do not notice a change it the buffer state stack. It makes the functions lexiacal\_analysis,
        tok\_to\_str, and clean\_lexer availible in its header file. clean\_lexer frees a lexer state struct and all
        memory owned by it. tok\_to\_str fills the first buffer argument with the string token name that corriponds
        to the token integer passed as the second argument.

    \section{Lexer}
        This lexer program uses the lex file to scan all input files provided passed in a string array to the lexical\_analysis function.
        It constructs a lexer\_state struct for each input file, which is meant to represent a single compilation unit. This files
        header also sets all the declarations for functions and global variables made in the lex file so I don't have to include the 
        c file generated by lex to reference these functions and variables.
        
        \subsection{Structs}

            \subsubsection{struct lexer\_state}
                This lexer\_state struct contains a hashmap of all the defined identifiers, it stores the value of the definition in a def\_map struct.
                The state also containes a array of all file names read as a part of this compilation unit. This allows me to have a valid copy of the
                file name that will not be freed until the state itself is freed. The state also containes a linked list of all the lexemes read 
                in this compilation unit. It also containers a pointer to the first token added to the state that must not be altered. This is 
                used to free all the lexemes stored in the state. The state also contains a string array of active files, treated as a stack,
                that is used during the lexing process to detect circular includes. This should be nulled out once the state is finished being processed. 
            
            \subsubsection{struct def\_map}
                The def\_map struct contains a pointer to the allocated copy of the identifier string used as the key to make freeing easier,
                as well as a linked list of tokens parsed from the definition directive line.

            \subsubsection{struct lexeme}
                The lexeme struct represents a single link in the token link list generated by my lexer.
                It contains links the the previous and next token, a uint\_8 for the token, a void pointer 
                for tokens that need to store dynamic data, as well as the filename and line number it was found at.

        \subsection{Static Functions}

            \subsubsection{add\_file}
                This function takes a lexer\_state and a file string. It then allocates memory for a copy of the file string
                and stores a pointer in the string array in the state. If the string array is full in the state it
                then reallocates the array to make room for it.

            \subsubsection{process\_token}
                This function takes a state and a lexeme. it then handles any specific additional logic needed for the token type.
                \begin{enumerate}
                    \item For string, character, type, integer, float, and hex tokens this involves converting and storing the value of the constant
                    in the value pointer in the lexeme. 
                    \item For include tokens this requires getting the filename to be included, opening the file,
                    and setting the lexer to read from the new file. I do this by creating a lex buffer and pushing it onto the buffer stack.
                    It then calls fill\_state to process this file. Once fill\_state completes it pops the empty buffer off the stack and returns
                    to continue parsing the original file.
                    \item For define tokens it calls add\_definition to add to the definition hashmap.\\
                    \item For ifdef and ifndef tokens it calls off to handle\_ifdef.
                    \item For all other directives it prints an error.
                    \item For identifier tokens it checks if it should replace to token with a previously defined value.
                    Otherwise it stores a copy of the string in the value of the lexeme.\\
                \end{enumerate}
                At the end if the lexer debug option is set and the file is going to be added to the state it prints the specified string in the
                assignment documentation. This function returns 0 if the token is ok to be added to the state and -1 if the token should
                be ignored(all directives are ignored).

            \subsubsection{add\_definition}
                This function gets the identifer for the definition and then processes all the lexemes for the definition
                value. It stores the lexemes in an array in the def\_map struct and exits upon getting a newline token. At the end
                it places the def\_map created into the hashmap for definitions on the state argument.

            \subsubsection{add\_lexeme}
                This function is pretty simple it just adds a lexeme to the linked list stored in the state. I wanted to unify 
                the logic for doing this because I was doing it in many places ealier (I have since reduced the number of places that
                add lexemes) and I wanted to make sure I was doing it right each time. I was having memory leaks at the start because I
                was doing it incorrectly in some locations.

            \subsubsection{resolve\_ident}
                This function is in charge of replacing identifiers with the defined values they corrispond to. Since this sidesteps the
                normal debug printing it also must print the tokens as it replaces them with a copy of the 
                lexeme stored in the definition map lexeme array. Since i normally get the text for each
                token from yytext in the printing and I don't store the text for types that are trivial I had to create
                a massive switch case. This handles every type of token and fakes the text for tokens that don't get their text
                saved in the lexeme. I don't like this pattern but I have already spent way to much time on this section of the compiler
                and this was the easiest way to get it to work so I could turn it in. If this code is still used in future sections
                I will probably redesign the process token to store the exact text in the value of all lexemes that are created for
                the definition array. This way i can just print the string stored in the value and I can treat almost all tokens the same.

            \subsubsection{handle\_ifdef}
                This function works for both ifdef and ifndef. It determines if the indentifier found by lex is defined
                then then either continues processing or ignores the following lexemes based on that. If it finds
                a else  directive it inverts the boolean variable it was using to change if it is ignoring or processing 
                the lexemes. If it finds a elif it then resets the boolean variable based on if the new identifier is defined.
                It exits when it finds and endif directive.

            \subsubsection{fill\_state}
                This function is runs yylex until a file is finished being processed. It returns when yylex returns 0. 
                It starts by adding the file string passed to the function to the state by calling add\_file.
                It then looks through the open file stack and determines if the current file is in the stack.
                If it is then it assumes there was a include cycle and returns and error. Otherwise it adds the file
                to the file stack.
                It sets up a new lexeme with all the information needed and then calls process\_token with the state and lexeme.

            \subsubsection{clean\_def\_map}
                This function is passed as a function point to the hashmap iterate function.
                This function is called with the def\_map struct stored in each node of the hashmap.
                it frees all the memory in the struct in order to clean up the allocated memory stored in the map.

    \section{Hashmap}
        I pulled this hashmap from a project on git at \url{https://github.com/petewarden/c\_hashmap}.
        It is just a basic hashmap.

\end{document}
